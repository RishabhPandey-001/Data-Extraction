Immigration Datawarehouse & AI-based recommendations

Client: A leading business school worldwide Industry Type: R&D Services: R&D, Innovation Organization Size: 100+ Objective of this project is to research and collect news article data sourcing from Canada, based on the keyword. There were 3 phases of the project. Phase 1 – Data collection and selection Data related to anyone coming to Canada (new comers) Data related to anyone coming to Canada (new comers) Canadian policy to new comers i.e. from any country to Canada Data containing News, press, think tanks, government policy documents, or research institutions releasing the news or press about The news source should be limited to Canada only Time span- 2005 to 2021 Output- Excel having URLs or the documents along with the source type, keywords, and date on which that article is posted. Data related to anyone coming to Canada (new comers) Data related to anyone coming to Canada (new comers) Canadian policy to new comers i.e. from any country to Canada Data containing News, press, think tanks, government policy documents, or research institutions releasing the news or press about The news source should be limited to Canada only Time span- 2005 to 2021 Output- Excel having URLs or the documents along with the source type, keywords, and date on which that article is posted. Phase 2 – Documents text data extraction Develop tool to collect and extract data from each URL. Clean and save the texts in the text documents Develop tool to collect and extract data from each URL. Clean and save the texts in the text documents Phase 3 – Textual Analysis Sentiment Analysis Analysis of readability Topic modelling Sentiment Analysis Analysis of readability Topic modelling We provide them with completed Phase 1 in an excel sheet and ongoing samples for Phase 2. Also work for Phase 3 has been started in between to complete the Project as soon as possible in a best way. There is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from Phase 2. Python, PyCharm, Jupyter Notebook, Microsoft Excel, Google Chrome is used to complete different phases of this project Python programming language is used to do Web Scraping, Automation, Data Engineering in this project. SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process. We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step. Figure 1 SDLC Iterative Waterfall Model Data scraping, cleaning, pre-processing and creating data pipelines are used in this project. We used the traditional way of storing the data i.e file systems. There were a lot of challenges we faced during the project execution. As on the internet, raw data is available to us. So, to search for the important data specifically related to Canada only, with a lot of keywords was a challenging part for us. Then, if we somehow manage to do the task by automating it upto some extent only, we are required to find the dates of the articles, news, think tanks, documents etc, that was also a challenging part. While working on Phase 2, we need to scrape the data from the URLs, so sometimes, the news articles were removed from the website, which we earlier took in our datasets which cause problems in extracting the data. Then cleaning the webpages was also challenge for us, because this project is for research, so data is important to us. So, it was difficult to take only that data from website which we require and are most important. Below are the points used to solve the above technical challenges- We used sitemaps of websites to find different articles that we require according to the keywords, manual research was done to find out which URL will solve the purpose. Manual checking of results of automation tools, that we created, was done. To find the dates of the articles, we wrote multiple regular expressions, that will find the match for the dates that we need, also manual checking was done after that. To scrape removed webpages, we used WayBack machine or google archives, which stores all the deleted webpages. To clean the data, we filtered out various HTML tags, classes, ids by using regex, manual research. Transforming Real Estate Investments with an Automated Stack shares Platform Empowering Careers: The Hirekingdom Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes Automated Job Data Import and Management Solution for Enhanced Efficiency How prepared is India to tackle a possible COVID-19 outbreak? How Coronavirus Impact on the Hospitality Industry Lessons from the past: Some key learnings relevant to the coronavirus... The Prospective Recipe of Success in the Age of Analytics What do you think is the lesson (or lessons) to be... Artificial intelligence in business: Separating the real from the hype Evaluating Logistic Regression Models